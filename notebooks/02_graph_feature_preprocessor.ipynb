{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import pandas as pd\n",
    "import time\n",
    "from snapml import GraphFeaturePreprocessor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"../config.json\") as f:\n",
    "    config = json.load(f)\n",
    "\n",
    "DATASET = config[\"dataset\"]\n",
    "PREPROCESSED_DATA = f\"../data/01-ibm-transactions-for-aml/preprocessed/{DATASET}-transactions\"\n",
    "WRITE_LOCATION = f\"../data/01-ibm-transactions-for-aml/feature_engineering/{DATASET}-enriched\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = pd.read_parquet(PREPROCESSED_DATA)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = data.sort_values(by=\"timestamp\", ascending=True)\n",
    "data[\"timestamp\"] = pd.to_datetime(data[\"timestamp\"])\n",
    "data[\"timestamp\"] = data[\"timestamp\"].values.astype(int) // 10**9\n",
    "min_timestamp = data[\"timestamp\"].min()\n",
    "data[\"timestamp\"] = data[\"timestamp\"] - min_timestamp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_to_preprocess = data[['transaction_id', \"source\", \"target\", \"timestamp\", \"amount\"]].copy()\n",
    "data_id = data['transaction_id'].values\n",
    "\n",
    "del data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "params = {\n",
    "    \"num_threads\": 12,                  # number of software threads to be used (important for performance)\n",
    "    \"time_window\": 24*3600,             # time window used if no pattern was specified (seconds)\n",
    "    \n",
    "    \"vertex_stats\": True,               # produce vertex statistics\n",
    "    \"vertex_stats_tw\": 24*3600,\n",
    "    # produce vertex statistics using the selected input columns (timestamp, source_amount, target_amount)\n",
    "    \"vertex_stats_cols\": [3,4],     \n",
    "    \n",
    "    # features: 0:fan,1:deg,2:ratio,3:avg,4:sum,5:min,6:max,7:median,8:var,9:skew,10:kurtosis\n",
    "    \"vertex_stats_feats\": [0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10],\n",
    "    #\"vertex_stats_feats\": [],\n",
    "    \n",
    "    # fan in/out parameters\n",
    "    \"fan\": True,\n",
    "    \"fan_tw\": 24*3600,\n",
    "    \"fan_bins\": [y+2 for y in range(16)],\n",
    "    \n",
    "    # in/out degree parameters\n",
    "    \"degree\": True,\n",
    "    \"degree_tw\": 24*3600,\n",
    "    \"degree_bins\": [y+1 for y in range(16)],\n",
    "    \n",
    "    # scatter gather parameters\n",
    "    \"scatter-gather\": True,   \n",
    "    \"scatter-gather_tw\": 6*3600,        # 6-hours\n",
    "    \"scatter-gather_bins\": [y+2 for y in range(16)],\n",
    "    \n",
    "    # temporal cycle parameters\n",
    "    \"temp-cycle\": True,\n",
    "    \"temp-cycle_tw\": 24*3600,\n",
    "    \"temp-cycle_bins\": [y+2 for y in range(12)],\n",
    "    \n",
    "    # length-constrained simple cycle parameters\n",
    "    \"lc-cycle\": True,\n",
    "    \"lc-cycle_tw\": 24*3600,\n",
    "    \"lc-cycle_len\": 5,\n",
    "    \"lc-cycle_bins\": [y+2 for y in range(16)]\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gp = GraphFeaturePreprocessor()\n",
    "gp.set_params(params)\n",
    "print(\"Graph feature preprocessor parameters: \", json.dumps(gp.get_params(), indent=4))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "src_dst_map = {}\n",
    "\n",
    "i = 0\n",
    "for src in data_to_preprocess[\"source\"].unique():\n",
    "    i += 1\n",
    "    src_dst_map[src] = i\n",
    "\n",
    "for dst in data_to_preprocess[\"target\"].unique():\n",
    "    if src_dst_map.get(dst) is None:\n",
    "        i += 1\n",
    "        src_dst_map[dst] = i\n",
    "\n",
    "data_to_preprocess.loc[:, \"source\"] = data_to_preprocess[\"source\"].map(src_dst_map)\n",
    "data_to_preprocess.loc[:, \"target\"] = data_to_preprocess[\"target\"].map(src_dst_map)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_to_preprocess.loc[:, 'id'] = range(len(data_to_preprocess))\n",
    "X = data_to_preprocess.loc[:, ['id', 'source', 'target', 'timestamp', 'amount']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Enriching the transactions with new graph features \")\n",
    "print(\"Raw dataset shape: \", X.shape)\n",
    "X = X.to_numpy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "start_time = time.time()\n",
    "\n",
    "X_train_enriched = gp.fit_transform(X.astype('float64'))\n",
    "\n",
    "end_time = time.time()\n",
    "runtime = end_time - start_time\n",
    "\n",
    "print(f\"Runtime: {runtime:.6f} seconds\")\n",
    "print(\"\\n\")\n",
    "print(\"Enriched dataset shape: \", X_train_enriched.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def print_enriched_transaction(transaction, params):\n",
    "    colnames = []\n",
    "\n",
    "    # add raw features names\n",
    "    colnames.append(\"transaction_id\")\n",
    "    colnames.append(\"source\")\n",
    "    colnames.append(\"target\")\n",
    "    colnames.append(\"timestamp\")\n",
    "    colnames.append(\"amount\")\n",
    "\n",
    "\n",
    "    # add features names for the graph patterns\n",
    "    for pattern in ['fan', 'degree', 'scatter-gather', 'temp-cycle', 'lc-cycle']:\n",
    "        if pattern in params:\n",
    "            if params[pattern]:\n",
    "                bins = len(params[pattern +'_bins'])\n",
    "                if pattern in ['fan', 'degree']:\n",
    "                    for i in range(bins-1):\n",
    "                        colnames.append(pattern+\"_in_bins_\"+str(params[pattern +'_bins'][i])+\"-\"+str(params[pattern +'_bins'][i+1]))\n",
    "                    colnames.append(pattern+\"_in_bins_\"+str(params[pattern +'_bins'][i+1])+\"-inf\")\n",
    "                    for i in range(bins-1):\n",
    "                        colnames.append(pattern+\"_out_bins_\"+str(params[pattern +'_bins'][i])+\"-\"+str(params[pattern +'_bins'][i+1]))\n",
    "                    colnames.append(pattern+\"_out_bins_\"+str(params[pattern +'_bins'][i+1])+\"-inf\")\n",
    "                else:\n",
    "                    for i in range(bins-1):\n",
    "                        colnames.append(pattern+\"_bins_\"+str(params[pattern +'_bins'][i])+\"-\"+str(params[pattern +'_bins'][i+1]))\n",
    "                    colnames.append(pattern+\"_bins_\"+str(params[pattern +'_bins'][i+1])+\"-inf\")\n",
    "\n",
    "    vert_feat_names = [\"fan\",\"deg\",\"ratio\",\"avg\",\"sum\",\"min\",\"max\",\"median\",\"var\",\"skew\",\"kurtosis\"]\n",
    "\n",
    "    # add features names for the vertex statistics\n",
    "    for orig in ['source', 'dest']:\n",
    "        for direction in ['out', 'in']:\n",
    "            # add fan, deg, and ratio features\n",
    "            for k in [0, 1, 2]:\n",
    "                if k in params[\"vertex_stats_feats\"]:\n",
    "                    feat_name = orig + \"_\" + vert_feat_names[k] + \"_\" + direction\n",
    "                    colnames.append(feat_name)\n",
    "            for col in params[\"vertex_stats_cols\"]:\n",
    "                # add avg, sum, min, max, median, var, skew, and kurtosis features\n",
    "                for k in [3, 4, 5, 6, 7, 8, 9, 10]:\n",
    "                    if k in params[\"vertex_stats_feats\"]:\n",
    "                        feat_name = orig + \"_\" + vert_feat_names[k] + \"_col\" + str(col) + \"_\" + direction\n",
    "                        colnames.append(feat_name)\n",
    "\n",
    "    return pd.DataFrame(transaction, columns=colnames)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "enriched_dataset = print_enriched_transaction(X_train_enriched, gp.get_params())\n",
    "enriched_dataset['transaction_id'] = data_id"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "enriched_dataset = enriched_dataset.loc[:, enriched_dataset.apply(pd.Series.nunique) != 1]\n",
    "print(json.dumps(enriched_dataset.columns.tolist(), indent=2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "enriched_dataset = enriched_dataset.drop(columns=['source', 'target', 'timestamp', 'amount'])\n",
    "print(\"Total columns\", len(enriched_dataset.columns))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "enriched_dataset.to_parquet(WRITE_LOCATION)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
