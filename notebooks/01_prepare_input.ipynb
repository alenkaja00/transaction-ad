{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import numpy as np\n",
    "import os\n",
    "import pandas as pd\n",
    "from pyspark.sql import types as st\n",
    "from pyspark.sql import functions as sf\n",
    "from pyspark import SparkConf\n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark.storagelevel import StorageLevel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "config = [\n",
    "    (\"spark.jars.packages\", \"graphframes:graphframes:0.8.3-spark3.5-s_2.13\"),\n",
    "    (\"spark.driver.memory\", \"8g\"),\n",
    "    (\"spark.worker.memory\", \"8g\"),\n",
    "    (\"spark.sql.session.timeZone\", \"UTC\")\n",
    "]\n",
    "spark = SparkSession.builder.appName(\"preprocessing\").config(conf=SparkConf().setAll(config)).getOrCreate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"../config.json\") as f:\n",
    "    config = json.load(f)\n",
    "\n",
    "DATASET = config[\"dataset\"]\n",
    "DATA_PATH = \"../data/01-ibm-transactions-for-aml\"\n",
    "\n",
    "RAW_DATA_PATH = os.path.join(DATA_PATH, \"raw\")\n",
    "PREPROCESSED_DATA_PATH = os.path.join(DATA_PATH, \"preprocessed\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "TIMESTAMP_FORMAT = \"yyyy/MM/dd HH:mm\"\n",
    "\n",
    "INPUT_DATA_FILE = os.path.join(RAW_DATA_PATH, f\"{DATASET}_Trans.csv\")\n",
    "INPUT_PATTERNS_FILE = os.path.join(RAW_DATA_PATH, f\"{DATASET}_Patterns.txt\")\n",
    "\n",
    "STAGED_TRANS_LOCATION = os.path.join(PREPROCESSED_DATA_PATH, f\"{DATASET}-transactions\")\n",
    "STAGED_DATA_FILE = os.path.join(PREPROCESSED_DATA_PATH, f\"{DATASET}-staged-transactions.csv\")\n",
    "STAGED_PATTERNS_LOCATION = os.path.join(PREPROCESSED_DATA_PATH, f\"{DATASET}-patterns\")\n",
    "STAGED_PATTERNS_CSV_LOCATION = os.path.join(STAGED_PATTERNS_LOCATION, f\"{DATASET}-patterns.txt\")\n",
    "STAGED_CASES_DATA_LOCATION = os.path.join(STAGED_PATTERNS_LOCATION, f\"{DATASET}-cases.parquet\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_id(value):\n",
    "    return f\"id-{hash(value)}\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "os.makedirs(os.path.dirname(STAGED_DATA_FILE), exist_ok=True)\n",
    "\n",
    "try:\n",
    "    os.remove(STAGED_DATA_FILE)\n",
    "except FileNotFoundError:\n",
    "    pass\n",
    "\n",
    "with open(INPUT_DATA_FILE) as input_file:\n",
    "    cnt = 0\n",
    "    lines = \"\"\n",
    "    for line in input_file:\n",
    "        cnt += 1\n",
    "        if cnt == 1:\n",
    "            continue\n",
    "        line = line.strip()\n",
    "        lines += f\"{get_id(line)},{line}\\n\"\n",
    "        if cnt % (1e6) == 0:\n",
    "            with open(STAGED_DATA_FILE, \"a\") as output_file:\n",
    "                output_file.write(lines)\n",
    "                lines = \"\"\n",
    "if lines:\n",
    "    lines = lines.strip()\n",
    "    with open(STAGED_DATA_FILE, \"a\") as output_file:\n",
    "        output_file.write(lines)\n",
    "        del lines"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "os.makedirs(STAGED_PATTERNS_LOCATION, exist_ok=True)\n",
    "\n",
    "try:\n",
    "    os.remove(STAGED_PATTERNS_CSV_LOCATION)\n",
    "except FileNotFoundError:\n",
    "    pass\n",
    "\n",
    "lines = \"\"\n",
    "with open(INPUT_PATTERNS_FILE) as input_file:\n",
    "    for line in input_file:\n",
    "        line = line.strip()\n",
    "        if line[:4].isnumeric():\n",
    "            lines += f\"{get_id(line)},{line}\\n\"\n",
    "        else:\n",
    "            lines += f\"{line}\\n\"\n",
    "lines = lines.strip()\n",
    "with open(STAGED_PATTERNS_CSV_LOCATION, \"a\") as output_file:\n",
    "    output_file.write(lines)\n",
    "    del lines"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "schema = st.StructType(\n",
    "    [\n",
    "        st.StructField(\"transaction_id\", st.StringType(), False),\n",
    "        st.StructField(\"timestamp\", st.TimestampType(), False),\n",
    "        st.StructField(\"source_bank\", st.StringType(), False),\n",
    "        st.StructField(\"source\", st.StringType(), False),\n",
    "        st.StructField(\"target_bank\", st.StringType(), False),\n",
    "        st.StructField(\"target\", st.StringType(), False),\n",
    "        st.StructField(\"received_amount\", st.FloatType(), False),\n",
    "        st.StructField(\"receiving_currency\", st.StringType(), False),\n",
    "        st.StructField(\"sent_amount\", st.FloatType(), False),\n",
    "        st.StructField(\"sending_currency\", st.StringType(), False),\n",
    "        st.StructField(\"format\", st.StringType(), False),\n",
    "        st.StructField(\"is_laundering\", st.IntegerType(), False),\n",
    "    ]\n",
    ")\n",
    "columns = [x.name for x in schema]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(STAGED_PATTERNS_CSV_LOCATION, \"r\") as fl:\n",
    "    patterns = fl.read()\n",
    "\n",
    "cases = []\n",
    "case_id = 0\n",
    "for pattern in patterns.split(\"\\n\\n\"):\n",
    "    case_id += 1\n",
    "    if not pattern.strip():\n",
    "        continue\n",
    "    pattern = pattern.split(\"\\n\")\n",
    "    name = pattern.pop(0).split(\" - \")[1]\n",
    "    category, sub_category = name, name\n",
    "    if \": \" in name:\n",
    "        category, sub_category = name.split(\": \")\n",
    "    pattern.pop()\n",
    "    case = pd.DataFrame([x.split(\",\") for x in pattern], columns=columns)\n",
    "    case.loc[:, \"id\"] = case_id\n",
    "    case.loc[:, \"type\"] = category\n",
    "    case.loc[:, \"sub_type\"] = sub_category\n",
    "    cases.append(case)\n",
    "cases = pd.concat(cases, ignore_index=True)\n",
    "cases = spark.createDataFrame(cases)\n",
    "cases = cases.withColumn(\"timestamp\", sf.to_timestamp(\"timestamp\", TIMESTAMP_FORMAT))\n",
    "cases = cases.select(\n",
    "    \"transaction_id\", \"id\", \"type\", \"sub_type\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "CURRENCY_MAPPING = {\n",
    "    \"Australian Dollar\": \"aud\",\n",
    "    \"Bitcoin\": \"btc\",\n",
    "    \"Brazil Real\": \"brl\",\n",
    "    \"Canadian Dollar\": \"cad\",\n",
    "    \"Euro\": \"eur\",\n",
    "    \"Mexican Peso\": \"mxn\",\n",
    "    \"Ruble\": \"rub\",\n",
    "    \"Rupee\": \"inr\",\n",
    "    \"Saudi Riyal\": \"sar\",\n",
    "    \"Shekel\": \"ils\",\n",
    "    \"Swiss Franc\": \"chf\",\n",
    "    \"UK Pound\": \"gbp\",\n",
    "    \"US Dollar\": \"usd\",\n",
    "    \"Yen\": \"jpy\",\n",
    "    \"Yuan\": \"cny\"\n",
    "}\n",
    "\n",
    "currency_code = sf.udf(lambda x: CURRENCY_MAPPING[x], st.StringType())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = spark.read.csv(STAGED_DATA_FILE, header=False, schema=schema, timestampFormat=TIMESTAMP_FORMAT)\n",
    "\n",
    "group_by = [\n",
    "    \"timestamp\",\n",
    "    \"source_bank\",\n",
    "    \"source\",\n",
    "    \"target_bank\",\n",
    "    \"target\",\n",
    "    \"receiving_currency\",\n",
    "    \"sending_currency\",\n",
    "    \"format\",\n",
    "]\n",
    "\n",
    "data = data.groupby(group_by).agg(\n",
    "    sf.first(\"transaction_id\").alias(\"transaction_id\"),\n",
    "    sf.collect_set(\"transaction_id\").alias(\"transaction_ids\"),\n",
    "    sf.sum(\"received_amount\").alias(\"received_amount\"), \n",
    "    sf.sum(\"sent_amount\").alias(\"sent_amount\"),\n",
    "    sf.max(\"is_laundering\").alias(\"is_laundering\"),\n",
    ")\n",
    "data = data.withColumn(\n",
    "    \"source_currency\", currency_code(sf.col(\"sending_currency\"))\n",
    ").withColumn(\n",
    "    \"target_currency\", currency_code(sf.col(\"receiving_currency\")),\n",
    ")\n",
    "data = data.join(cases, on=\"transaction_id\", how=\"left\").repartition(128, \"transaction_id\")\n",
    "data = data.select(\n",
    "    \"transaction_id\",\n",
    "    \"transaction_ids\",\n",
    "    \"timestamp\",\n",
    "    sf.concat(sf.col(\"source\"), sf.lit(\"-\"), sf.col(\"source_currency\")).alias(\"source\"),\n",
    "    sf.concat(sf.col(\"target\"), sf.lit(\"-\"), sf.col(\"target_currency\")).alias(\"target\"),\n",
    "    \"source_bank\",\n",
    "    \"target_bank\",\n",
    "    \"source_currency\",\n",
    "    \"target_currency\",\n",
    "    sf.col(\"sent_amount\").alias(\"source_amount\"),\n",
    "    sf.col(\"received_amount\").alias(\"target_amount\"),\n",
    "    \"format\",\n",
    "    \"is_laundering\",\n",
    ").persist(StorageLevel.DISK_ONLY)\n",
    "data.count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data.select(sf.explode(\"transaction_ids\")).count() - data.count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cases_data = cases.join(\n",
    "    data.withColumnRenamed(\"transaction_id\", \"x\").drop(*cases.columns).select(\n",
    "        sf.explode(\"transaction_ids\").alias(\"transaction_id\"), \"*\"\n",
    "    ),\n",
    "    on=\"transaction_id\",\n",
    "    how=\"left\",\n",
    ").drop(\"is_laundering\", \"transaction_id\", \"transaction_ids\").withColumnRenamed(\"x\", \"transaction_id\")\n",
    "cases_data.toPandas().to_parquet(STAGED_CASES_DATA_LOCATION)\n",
    "cases_data = pd.read_parquet(STAGED_CASES_DATA_LOCATION)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = data.drop(\"transaction_ids\")\n",
    "\n",
    "data.write.mode(\"overwrite\").parquet(STAGED_TRANS_LOCATION)\n",
    "data = spark.read.parquet(STAGED_TRANS_LOCATION)\n",
    "\n",
    "os.remove(STAGED_DATA_FILE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = data.withColumn(\"date\", sf.to_date(\"timestamp\"))\n",
    "data = data.orderBy(\"timestamp\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = data.withColumn(\n",
    "    \"converted_amount\",\n",
    "    sf.when(sf.col(\"source_currency\") == \"usd\", sf.col(\"source_amount\"))\n",
    "    .when(sf.col(\"target_currency\") == \"usd\", sf.col(\"target_amount\"))\n",
    "    .otherwise(0)\n",
    ")\n",
    "\n",
    "unique_currencies = data.select(sf.col(\"source_currency\").alias(\"currency\")).union(\n",
    "    data.select(sf.col(\"target_currency\").alias(\"currency\"))\n",
    ").distinct().collect()\n",
    "unique_currencies = [row.currency for row in unique_currencies]\n",
    "\n",
    "dates = data.select(\"date\").distinct().orderBy(\"date\").collect()\n",
    "dates = sorted([row.date for row in dates])\n",
    "\n",
    "source_usd = data.filter(sf.col(\"source_currency\") == \"usd\").filter(sf.col(\"target_currency\") != \"usd\")\\\n",
    "    .withColumn(\"rate\", sf.col(\"target_amount\") / sf.col(\"source_amount\"))\n",
    "\n",
    "target_usd = data.filter(sf.col(\"target_currency\") == \"usd\").filter(sf.col(\"source_currency\") != \"usd\")\\\n",
    "    .withColumn(\"rate\", sf.col(\"source_amount\") / sf.col(\"target_amount\"))\n",
    "\n",
    "source_usd.cache()\n",
    "target_usd.cache()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "curr_dict = {date: {curr: 1 if curr == 'usd' else 0 for curr in unique_currencies} for date in dates}\n",
    "\n",
    "combined_df = source_usd.select(sf.col(\"date\"), sf.col(\"target_currency\").alias(\"currency\"), sf.col(\"rate\")) \\\n",
    "    .union(target_usd.select(sf.col(\"date\"), sf.col(\"source_currency\").alias(\"currency\"), sf.col(\"rate\")))\n",
    "\n",
    "mean_rates = combined_df.groupBy(\"date\", \"currency\").agg(sf.mean(\"rate\").alias(\"mean_rate\"))\n",
    "\n",
    "rate_dict = {(row.date, row.currency): row.mean_rate for row in mean_rates.collect()}\n",
    "\n",
    "for date in dates:\n",
    "    for currency in unique_currencies:\n",
    "        if curr_dict[date][currency] == 0:\n",
    "            curr_dict[date][currency] = rate_dict.get((date, currency), 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.DataFrame(curr_dict).T\n",
    "df.sort_index(inplace=True)\n",
    "df.replace(0, np.nan, inplace=True)\n",
    "df.ffill(axis=0, inplace=True)\n",
    "curr_dict = df.T.to_dict()\n",
    "curr_dict = {str(key): value for key, value in curr_dict.items()}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = data.withColumn(\"date_str\", sf.date_format(sf.col(\"date\"), \"yyyy-MM-dd\"))\n",
    "\n",
    "broadcast_curr_dict = spark.sparkContext.broadcast(curr_dict)\n",
    "\n",
    "def get_rate(date, currency):\n",
    "    if currency =='usd':\n",
    "        return 1.0\n",
    "    return broadcast_curr_dict.value.get(date, {}).get(currency, 0)\n",
    "rate_udf = sf.udf(get_rate, st.DoubleType())\n",
    "\n",
    "data = data.withColumn(\"rate\", rate_udf(sf.col(\"date_str\"), sf.col(\"source_currency\")))\n",
    "data = data.withColumn(\"converted_amount\", sf.col(\"source_amount\") / sf.col(\"rate\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = data.drop(\"date\",\"date_str\", \"rate\")\n",
    "data = data.withColumnRenamed(\"converted_amount\", \"amount\")\n",
    "data = data.withColumn(\"amount\", sf.round(data.amount, 6))\n",
    "data = data.withColumn(\"timestamp\", sf.date_format(sf.col(\"timestamp\"), \"yyyy-MM-dd HH:mm\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data.write.mode(\"overwrite\").parquet(STAGED_TRANS_LOCATION)\n",
    "spark.stop()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
