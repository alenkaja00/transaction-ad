{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import networkx as nx\n",
    "from pyspark.sql import functions as sf\n",
    "from pyspark import SparkConf\n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql import Window"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"../config.json\") as f:\n",
    "    config = json.load(f)\n",
    "\n",
    "DATASET = config[\"dataset\"]\n",
    "PREPROCESSED_DATA = f\"../data/01-ibm-transactions-for-aml/preprocessed/{DATASET}-transactions\"\n",
    "WRITE_LOCATION = f\"../data/01-ibm-transactions-for-aml/feature_engineering/{DATASET}-features\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "config = [\n",
    "    (\"spark.jars.packages\", \"graphframes:graphframes:0.8.3-spark3.5-s_2.13\"),\n",
    "    (\"spark.driver.memory\", \"8g\"),\n",
    "    (\"spark.worker.memory\", \"8g\"),\n",
    "]\n",
    "spark = SparkSession.builder.appName(\"feature_engineering\").config(conf=SparkConf().setAll(config)).getOrCreate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def rename_columns(dataframe, names):\n",
    "    for name, new_name in names.items():\n",
    "        dataframe = dataframe.withColumnRenamed(name, new_name)\n",
    "    return dataframe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = spark.read.parquet(PREPROCESSED_DATA)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "balance_data = data.select(sf.col(\"source\").alias(\"account\"), \n",
    "                        (-sf.col(\"amount\")).alias(\"balance\"),\n",
    "                        sf.col(\"timestamp\"), sf.col(\"transaction_id\")\n",
    "                        ).union(\n",
    "                            data.select(sf.col(\"target\").alias(\"account\"),\n",
    "                                        sf.col(\"amount\").alias(\"balance\"),\n",
    "                                        sf.col(\"timestamp\"), sf.col(\"transaction_id\")))\n",
    "\n",
    "balance_data = balance_data.repartition(\"account\")\n",
    "\n",
    "windowval = (Window.partitionBy(\"account\").orderBy(\"timestamp\").rangeBetween(Window.unboundedPreceding, 0))\n",
    "balance_data = balance_data.withColumn(\"cum_sum\", sf.sum(\"balance\").over(windowval))\n",
    "\n",
    "df_negative = balance_data.filter(sf.col(\"balance\") < 0) \\\n",
    "    .withColumnRenamed(\"account\", \"source\") \\\n",
    "    .withColumnRenamed(\"balance\", \"src_increment\") \\\n",
    "    .withColumnRenamed(\"cum_sum\", \"new_src_balance\")\n",
    "\n",
    "df_positive = balance_data.filter(sf.col(\"balance\") > 0) \\\n",
    "    .withColumnRenamed(\"account\", \"target\") \\\n",
    "    .withColumnRenamed(\"balance\", \"dst_increment\") \\\n",
    "    .withColumnRenamed(\"cum_sum\", \"new_dst_balance\")\n",
    "\n",
    "data = data.repartition(\"transaction_id\", \"timestamp\").join(df_negative, on=[\"transaction_id\", \"timestamp\", \"source\"], how=\"left\") \\\n",
    "        .join(df_positive, on=[\"transaction_id\", \"timestamp\", \"target\"], how=\"left\")\n",
    "\n",
    "data = data.withColumn(\"src_increment\", sf.round(sf.col(\"src_increment\"), 2)) \\\n",
    "        .withColumn(\"dst_increment\", sf.round(sf.col(\"dst_increment\"), 2)) \\\n",
    "        .withColumn(\"new_src_balance\", sf.round(sf.col(\"new_src_balance\"), 2)) \\\n",
    "        .withColumn(\"new_dst_balance\", sf.round(sf.col(\"new_dst_balance\"), 2))\n",
    "\n",
    "data = data.withColumn('src_increment', sf.when(sf.col('source') == sf.col('target'), 0).otherwise(sf.col('src_increment'))) \\\n",
    "            .withColumn('dst_increment', sf.when(sf.col('source') == sf.col('target'), 0).otherwise(sf.col('dst_increment'))) \\\n",
    "            .withColumn('old_src_balance', sf.col('new_src_balance') - sf.col('src_increment')) \\\n",
    "            .withColumn('old_dst_balance', sf.col('new_dst_balance') - sf.col('dst_increment')) \\\n",
    "            .withColumn('new_src_balance', sf.when(sf.col('source') == sf.col('target'), sf.col('new_dst_balance')).otherwise(sf.col('new_src_balance'))) \\\n",
    "            .withColumn('old_src_balance', sf.when(sf.col('source') == sf.col('target'), sf.col('old_dst_balance')).otherwise(sf.col('old_src_balance')))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sub_df = data.select('source', 'target', 'amount')\n",
    "\n",
    "src_group = sub_df.groupBy('source').agg(\n",
    "                        sf.sum('amount').alias('total_sent'),\n",
    "                        sf.avg('amount').alias('avg_sent'),\n",
    "                        sf.stddev('amount').alias('stddev_sent'),\n",
    "                        sf.countDistinct('target').alias('src_total_counterparties'))\n",
    "\n",
    "dst_group = sub_df.groupBy('target').agg(\n",
    "                        sf.sum('amount').alias('total_received'),\n",
    "                        sf.avg('amount').alias('avg_received'),\n",
    "                        sf.stddev('amount').alias('stddev_received'),\n",
    "                        sf.countDistinct('source').alias('dst_total_counterparties'))\n",
    "\n",
    "data = (data.join(src_group, on='source', how='left')\n",
    "            .join(dst_group, on='target', how='left'))\n",
    "\n",
    "data = data.withColumn('percentage_of_total_sent', (sf.col('amount') / sf.col('total_sent')) * 100) \\\n",
    "            .withColumn('percentage_of_total_received', (sf.col('amount') / sf.col('total_received')) * 100)\\\n",
    "            .withColumn('percentage_of_avg_sent', (sf.col('amount') / sf.col('avg_sent')) * 100)\\\n",
    "            .withColumn('percentage_of_avg_received', (sf.col('amount') / sf.col('avg_received')) * 100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data.write.mode(\"overwrite\").parquet(WRITE_LOCATION)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = spark.read.parquet(WRITE_LOCATION)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = data.withColumn('timestamp', sf.to_timestamp(sf.col('timestamp'), 'yyyy-MM-dd HH:mm'))\\\n",
    "            .withColumn('day', sf.dayofmonth(sf.col('timestamp')))\\\n",
    "            .withColumn('week', sf.weekofyear(sf.col('timestamp')))\\\n",
    "            .withColumn('date', sf.to_date(sf.col('timestamp')))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "src_daily = data.groupBy('source', 'day').agg(\n",
    "                        sf.sum('amount').alias('src_daily_paid_amount'),\n",
    "                        sf.count('*').alias('src_daily_trans_count'),\n",
    "                        sf.avg('amount').alias('src_daily_avg_paid_amount'),\n",
    "                        sf.stddev('amount').alias('src_daily_std_amount'),\n",
    "                        sf.countDistinct('target').alias('src_daily_counterparties'))\n",
    "\n",
    "dst_daily = data.groupBy('target', 'day').agg(\n",
    "                        sf.sum('amount').alias('dst_daily_received_amount'),\n",
    "                        sf.count('*').alias('dst_daily_trans_count'),\n",
    "                        sf.avg('amount').alias('dst_daily_avg_received_amount'),\n",
    "                        sf.stddev('amount').alias('dst_daily_std_amount'),\n",
    "                        sf.countDistinct('source').alias('dst_daily_counterparties'))\n",
    "\n",
    "data = (data.join(src_daily, on=['source', 'day'], how='left')\n",
    "            .join(dst_daily, on=['target', 'day'], how='left'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "EXPANSION_DAYS = 10\n",
    "ROLLING_D = 5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_to_expand = data.select(\"source\", \"target\", \"amount\", \"timestamp\", \"transaction_id\", \"date\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def expand_dates(data, mode, direction):\n",
    "    date_range_df = data.groupBy(mode).agg(\n",
    "        sf.min('date').alias('min_date'),\n",
    "        sf.max('date').alias('max_date')\n",
    "    )\n",
    "    if direction == \"back\":\n",
    "        date_range_df = date_range_df.withColumn(\n",
    "            'date_sequence',\n",
    "            sf.explode(\n",
    "                sf.sequence(\n",
    "                    sf.date_add(sf.col('min_date'), -EXPANSION_DAYS),\n",
    "                    sf.col('max_date'),\n",
    "                    sf.expr('INTERVAL 1 DAY')\n",
    "                )\n",
    "            )\n",
    "        ).select(\n",
    "            sf.col(mode),\n",
    "            sf.col('date_sequence').alias('date')\n",
    "        )\n",
    "    elif direction == \"fwd\":\n",
    "        date_range_df = date_range_df.withColumn(\n",
    "            'date_sequence',\n",
    "            sf.explode(\n",
    "                sf.sequence(\n",
    "                    sf.col('min_date'),\n",
    "                    sf.date_add(sf.col('max_date'), EXPANSION_DAYS),\n",
    "                    sf.expr('INTERVAL 1 DAY')\n",
    "                )\n",
    "            )\n",
    "        ).select(\n",
    "            sf.col(mode),\n",
    "            sf.col('date_sequence').alias('date')\n",
    "        )\n",
    "\n",
    "    return date_range_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "src_daily_totals = df_to_expand.groupBy('source', 'date').agg(sf.sum('amount').alias('out_amount'))\n",
    "dst_daily_totals = df_to_expand.groupBy('target', 'date').agg(sf.sum('amount').alias('in_amount'))\n",
    "\n",
    "src_dates_back = expand_dates(src_daily_totals, \"source\", \"back\")\n",
    "dst_dates_back = expand_dates(dst_daily_totals, \"target\", \"back\")\n",
    "\n",
    "src_dates_fwd = expand_dates(src_daily_totals, 'source', \"fwd\")\n",
    "dst_dates_fwd = expand_dates(dst_daily_totals, 'target', \"fwd\")\n",
    "\n",
    "src_daily_totals_back = src_daily_totals.join(src_dates_back, on=['source', 'date'], how='right').fillna(0)\n",
    "dst_daily_totals_back = dst_daily_totals.join(dst_dates_back, on=['target', 'date'], how='right').fillna(0)\n",
    "\n",
    "src_daily_totals_fwd = src_daily_totals.join(src_dates_fwd, on=['source', 'date'], how='right').fillna(0)\n",
    "dst_daily_totals_fwd = dst_daily_totals.join(dst_dates_fwd, on=['target', 'date'], how='right').fillna(0)\n",
    "\n",
    "del src_dates_back, dst_dates_back, src_dates_fwd, dst_dates_fwd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "back_rolling = f'sum_last_{ROLLING_D}_d'\n",
    "back_percent =  f'percentage_of_total_last_{ROLLING_D}_d'\n",
    "\n",
    "fwd_rolling = f'sum_next_{ROLLING_D}_d'\n",
    "fwd_percent =  f'percentage_of_total_next_{ROLLING_D}_d'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_percent(data, partition_col, amount_col, rolling_col_name, percent_col_name, ROLLING_D, direction='back'):\n",
    "    if direction == 'back':\n",
    "        window_spec = Window.partitionBy(partition_col).orderBy('date').rowsBetween(-(ROLLING_D - 1), 0)\n",
    "    elif direction == 'fwd':\n",
    "        window_spec = Window.partitionBy(partition_col).orderBy('date').rowsBetween(0, ROLLING_D - 1)\n",
    "    \n",
    "    data = data.withColumn(\n",
    "        rolling_col_name,\n",
    "        sf.sum(amount_col).over(window_spec)\n",
    "    )\n",
    "    data = data.withColumn(\n",
    "        percent_col_name,\n",
    "        (sf.col(amount_col) / sf.col(rolling_col_name) * 100)\n",
    "    )\n",
    "    data = data.filter(sf.col(percent_col_name).isNotNull())\n",
    "    \n",
    "    return data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "src_daily_totals_back = calculate_percent(\n",
    "    src_daily_totals_back,\n",
    "    partition_col='source',\n",
    "    amount_col='out_amount',\n",
    "    rolling_col_name=back_rolling,\n",
    "    percent_col_name=back_percent,\n",
    "    ROLLING_D=ROLLING_D,\n",
    "    direction='back'\n",
    ")\n",
    "\n",
    "dst_daily_totals_back = calculate_percent(\n",
    "    dst_daily_totals_back,\n",
    "    partition_col='target',\n",
    "    amount_col='in_amount',\n",
    "    rolling_col_name=back_rolling,\n",
    "    percent_col_name=back_percent,\n",
    "    ROLLING_D=ROLLING_D,\n",
    "    direction='back'\n",
    ")\n",
    "\n",
    "src_daily_totals_fwd = calculate_percent(\n",
    "    src_daily_totals_fwd,\n",
    "    partition_col='source',\n",
    "    amount_col='out_amount',\n",
    "    rolling_col_name=fwd_rolling,\n",
    "    percent_col_name=fwd_percent,\n",
    "    ROLLING_D=ROLLING_D,\n",
    "    direction='fwd'\n",
    ")\n",
    "\n",
    "dst_daily_totals_fwd = calculate_percent(\n",
    "    dst_daily_totals_fwd,\n",
    "    partition_col='target',\n",
    "    amount_col='in_amount',\n",
    "    rolling_col_name=fwd_rolling,\n",
    "    percent_col_name=fwd_percent,\n",
    "    ROLLING_D=ROLLING_D,\n",
    "    direction='fwd'\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "src_daily_totals = (src_daily_totals\n",
    "                    .join(src_daily_totals_back, on=['source', 'date'], how='left')\n",
    "                    .join(src_daily_totals_fwd, on=['source', 'date'], how='left')\n",
    "                    .drop('out_amount', f'sum_last_{ROLLING_D}_d', f'sum_next_{ROLLING_D}_d'))\n",
    "\n",
    "dst_daily_totals = (dst_daily_totals\n",
    "                    .join(dst_daily_totals_back, on=['target', 'date'], how='left')\n",
    "                    .join(dst_daily_totals_fwd, on=['target', 'date'], how='left')\n",
    "                    .drop('in_amount', f'sum_last_{ROLLING_D}_d', f'sum_next_{ROLLING_D}_d'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mapping_src = {f'percentage_of_total_last_{ROLLING_D}_d': f'percentage_of_total_last_{ROLLING_D}_d_sent',\n",
    "           f'percentage_of_total_next_{ROLLING_D}_d': f'percentage_of_total_next_{ROLLING_D}_d_sent'}\n",
    "\n",
    "mapping_dst = {f'percentage_of_total_last_{ROLLING_D}_d': f'percentage_of_total_last_{ROLLING_D}_d_received',\n",
    "              f'percentage_of_total_next_{ROLLING_D}_d': f'percentage_of_total_next_{ROLLING_D}_d_received'}\n",
    "\n",
    "src_daily_totals = rename_columns(src_daily_totals, mapping_src)\n",
    "dst_daily_totals = rename_columns(dst_daily_totals, mapping_dst)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = ( data\n",
    "        .join(src_daily_totals, on=['source', 'date'], how='left')\n",
    "        .join(dst_daily_totals, on=['target', 'date'], how='left'))\n",
    "\n",
    "del src_daily_totals, dst_daily_totals"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = data.drop('date')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = data.withColumn(f'percentage_of_total_last_{ROLLING_D}_d_sent', sf.col('amount') / sf.col('src_daily_paid_amount') * sf.col(f'percentage_of_total_last_{ROLLING_D}_d_sent'))\\\n",
    "            .withColumn(f'percentage_of_total_last_{ROLLING_D}_d_received', sf.col('amount') / sf.col('dst_daily_received_amount') * sf.col(f'percentage_of_total_last_{ROLLING_D}_d_received'))\\\n",
    "            .withColumn(f'percentage_of_total_next_{ROLLING_D}_d_sent', sf.col('amount') / sf.col('src_daily_paid_amount') * sf.col(f'percentage_of_total_next_{ROLLING_D}_d_sent'))\\\n",
    "            .withColumn(f'percentage_of_total_next_{ROLLING_D}_d_received', sf.col('amount') / sf.col('dst_daily_received_amount') * sf.col(f'percentage_of_total_next_{ROLLING_D}_d_received'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data.write.mode(\"overwrite\").parquet(WRITE_LOCATION)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = spark.read.parquet(WRITE_LOCATION)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from_to_count = data.groupBy('source_bank', 'target_bank').agg(sf.count('*').alias('src_bank_as_src_with_this_dst_bank'))\n",
    "from_total_count = data.groupBy('source_bank').agg(sf.count('*').alias('src_bank_as_src'))\n",
    "to_from_count = data.groupBy('target_bank', 'source_bank').agg(sf.count('*').alias('dst_bank_as_dst_with_this_src_bank'))\n",
    "to_total_count = data.groupBy('target_bank').agg(sf.count('*').alias('dst_bank_as_dst'))\n",
    "\n",
    "ratio_from_df = from_to_count.join(from_total_count, on='source_bank')\n",
    "ratio_to_df = to_from_count.join(to_total_count, on='target_bank')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ratio_from_df = ratio_from_df.withColumn('from_to_ratio', (sf.col('src_bank_as_src_with_this_dst_bank') / sf.col('src_bank_as_src')) * 100)\n",
    "ratio_from_df = ratio_from_df.drop('src_bank_as_src_with_this_dst_bank', 'src_bank_as_src')\n",
    "\n",
    "ratio_to_df = ratio_to_df.withColumn('to_from_ratio',(sf.col('dst_bank_as_dst_with_this_src_bank') / sf.col('dst_bank_as_dst')) * 100)\n",
    "ratio_to_df = ratio_to_df.drop('dst_bank_as_dst_with_this_src_bank', 'dst_bank_as_dst')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = (data\n",
    "        .join(ratio_from_df, on=['source_bank', 'target_bank'], how='left')\n",
    "        .join(ratio_to_df, on=['target_bank', 'source_bank'], how='left')) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "del from_to_count, from_total_count, to_from_count, to_total_count, ratio_from_df, ratio_to_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = data.withColumn('is_cash', sf.when(sf.col('format') == 'Cash', 1).otherwise(0))\n",
    "\n",
    "# TODO: add the following features in other datasets\n",
    "# is_cash_credit, is_cash_debit"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ROUND_AMOUNT_MULTIPLIER = 50\n",
    "ROUND_AMOUNT_SLACK = 20\n",
    "ROUND_AMOUNT_MIN_VOLUME = 1000\n",
    "\n",
    "def is_round_amount(value):\n",
    "    if value is None:\n",
    "        return 0\n",
    "    return int((value > ROUND_AMOUNT_MIN_VOLUME) and\\\n",
    "    (\n",
    "        (value % ROUND_AMOUNT_MULTIPLIER >= ROUND_AMOUNT_MULTIPLIER - ROUND_AMOUNT_SLACK)\n",
    "        or \n",
    "        (value % ROUND_AMOUNT_MULTIPLIER <= ROUND_AMOUNT_SLACK)\n",
    "    ))\n",
    "\n",
    "is_round_amount_udf = sf.udf(is_round_amount)\n",
    "data = data.withColumn('is_round_amount', is_round_amount_udf(sf.col('amount')))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = data.withColumn('is_international', sf.when(sf.col('source_currency') != sf.col('target_currency'), 1).otherwise(0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = data.withColumn('is_different_bank', sf.when(sf.col('source_bank') != sf.col('target_bank'), 1).otherwise(0))\\\n",
    "            .withColumn('same_account_same_bank', sf.when((sf.col('source') == sf.col('target')) & (sf.col('source_bank') == sf.col('target_bank')), 1).otherwise(0))\\\n",
    "            .withColumn('same_bank_diff_account', sf.when((sf.col('source') != sf.col('target')) & (sf.col('source_bank') == sf.col('target_bank')), 1).otherwise(0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "src_dst_day = data.groupBy('source', 'target', 'day').agg(\n",
    "                                    sf.count('*').alias('daily_trans_count_counterparty'),\n",
    "                                    sf.avg('amount').alias('daily_avg_paid_amount_counterparty'))\n",
    "\n",
    "data = data.join(src_dst_day, on=['source', 'target', 'day'], how='left')\n",
    "del src_dst_day"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = data.drop('day')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "total_interactions = data.groupBy('source', 'target').count().withColumnRenamed('count', 'total_interactions')\n",
    "weekly_interactions = data.groupBy('source', 'target', 'week').count().withColumnRenamed('count', 'weekly_interactions')\n",
    "\n",
    "data = (data.join(total_interactions, on=['source', 'target'], how='left')\n",
    "        .join(weekly_interactions, on=['source', 'target', 'week'], how='left'))\n",
    "del total_interactions, weekly_interactions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "src_week = data.groupBy('source', 'week').agg(\n",
    "        sf.stddev('amount').alias('src_weekly_std_amount'),\n",
    "        sf.countDistinct('target').alias('src_weekly_counterparties'))\n",
    "\n",
    "target_week = data.groupBy('target', 'week').agg(\n",
    "        sf.stddev('amount').alias('dst_weekly_std_amount'),\n",
    "        sf.countDistinct('source').alias('dst_weekly_counterparties'))\n",
    "\n",
    "data = (data.join(src_week, on=['source', 'week'], how='left')\n",
    "        .join(target_week, on=['target', 'week'], how='left'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = data.withColumn('src_percentage_of_counterparty', (1 / sf.col('src_total_counterparties')) * 100)\\\n",
    "            .withColumn('dst_percentage_of_counterparty', (1 / sf.col('dst_total_counterparties')) * 100)\\\n",
    "            .withColumn('src_daily_percentage_of_counterparty', (1 / sf.col('src_daily_counterparties')) * 100)\\\n",
    "            .withColumn('dst_daily_percentage_of_counterparty', (1 / sf.col('dst_daily_counterparties')) * 100)\\\n",
    "            .withColumn('src_weekly_percentage_of_counterparty', (1 / sf.col('src_weekly_counterparties')) * 100)\\\n",
    "            .withColumn('dst_weekly_percentage_of_counterparty', (1 / sf.col('dst_weekly_counterparties')) * 100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "weekly_sent = data.groupBy('source', 'week').agg(sf.sum('amount').alias('weekly_total_sent'))\n",
    "weekly_received = data.groupBy('target', 'week').agg(sf.sum('amount').alias('weekly_total_received'))\n",
    "\n",
    "data = (data.join(weekly_sent, on=['source', 'week'], how='left')\n",
    "            .join(weekly_received, on=['target', 'week'], how='left'))\n",
    "\n",
    "del weekly_sent, weekly_received\n",
    "\n",
    "data = data.withColumn('percentage_weekly_sent', (sf.col('amount') / sf.col('weekly_total_sent'))* 100)\n",
    "data = data.withColumn('percentage_weekly_received', (sf.col('amount') / sf.col('weekly_total_received')) * 100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = data.drop('week')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = data.withColumn('percentage_daily_sent', (sf.col('amount') / sf.col('src_daily_paid_amount')) * 100)\n",
    "data = data.withColumn('percentage_daily_received', (sf.col('amount') / sf.col('dst_daily_received_amount')) * 100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = data.withColumn('percentage_of_src_balance', sf.when(sf.col('old_src_balance') == 0, 0).otherwise(sf.col('amount') / sf.col('old_src_balance') * 100))\n",
    "data = data.withColumn('percentage_of_dst_balance', sf.when(sf.col('new_dst_balance') == 0, 0).otherwise(sf.col('amount') / sf.col('new_dst_balance') * 100))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "src_interactions = data.groupBy('source').agg(sf.count('*').alias('src_interactions'))\n",
    "dst_interactions = data.groupBy('target').agg(sf.count('*').alias('dst_interactions'))\n",
    "\n",
    "data = (data.join(src_interactions, on='source', how='left')\n",
    "            .join(dst_interactions, on='target', how='left'))\n",
    "del src_interactions, dst_interactions\n",
    "\n",
    "data = data.withColumn('src_percentage_of_interactions', sf.col('total_interactions') / sf.col('src_interactions') * 100)\n",
    "data = data.withColumn('dst_percentage_of_interactions', sf.col('total_interactions') / sf.col('dst_interactions') * 100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "source_window = Window.partitionBy(\"source\").orderBy(\"timestamp\")\n",
    "target_window = Window.partitionBy(\"target\").orderBy(\"timestamp\")\n",
    "\n",
    "data = data.withColumn(\"src_time_diff\", \n",
    "                       (sf.unix_timestamp(sf.col(\"timestamp\")) - sf.unix_timestamp(sf.lag(\"timestamp\", 1).over(source_window))).cast(\"double\"))\\\n",
    "            .withColumn(\"dst_time_diff\", \n",
    "                       (sf.unix_timestamp(sf.col(\"timestamp\")) - sf.unix_timestamp(sf.lag(\"timestamp\", 1).over(target_window))).cast(\"double\"))\n",
    "data = data.na.fill({\"src_time_diff\": 0, \"dst_time_diff\": 0})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = data.fillna(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data.write.mode(\"overwrite\").parquet(WRITE_LOCATION)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = spark.read.parquet(WRITE_LOCATION)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_aggregated_all = data.groupBy(\"source\", \"target\").agg(\n",
    "        sf.sum(\"amount\").alias(\"sent_amount\"),\n",
    "        sf.sum(\"amount\").alias(\"received_amount\")\n",
    ")\n",
    "\n",
    "mapping_source = data_aggregated_all.groupBy(\"source\").agg(sf.sum(\"sent_amount\").alias(\"total_sent_by_source\"))\n",
    "mapping_target = data_aggregated_all.groupBy(\"target\").agg(sf.sum(\"received_amount\").alias(\"total_received_by_target\"))\n",
    "\n",
    "data_aggregated_all = data_aggregated_all.join(mapping_source, on=\"source\", how=\"left\")\n",
    "data_aggregated_all = data_aggregated_all.join(mapping_target, on=\"target\", how=\"left\")\n",
    "\n",
    "data_aggregated_all = data_aggregated_all.withColumn(\n",
    "    \"weight\",\n",
    "    (sf.col(\"sent_amount\") / sf.col(\"total_sent_by_source\"))+\n",
    "    (sf.col(\"received_amount\") / sf.col(\"total_received_by_target\")))\n",
    "\n",
    "edges = data_aggregated_all.select(\"source\", \"target\", \"weight\").toPandas()\n",
    "\n",
    "\n",
    "\n",
    "G = nx.from_pandas_edgelist(edges, source=\"source\", target=\"target\", edge_attr=\"weight\", create_using=nx.DiGraph)\n",
    "pagerank = nx.pagerank(G)\n",
    "degree_centrality = nx.degree_centrality(G)\n",
    "\n",
    "pagerank_df = spark.createDataFrame([(k, v) for k, v in pagerank.items()], [\"node\", \"pagerank\"])\n",
    "degree_centrality_df = spark.createDataFrame([(k, v) for k, v in degree_centrality.items()], [\"node\", \"degree_centrality\"])\n",
    "\n",
    "data = data.join(pagerank_df.withColumnRenamed(\"node\", \"source\").withColumnRenamed(\"pagerank\", \"src_pr\"), on=\"source\", how=\"left\")\n",
    "data = data.join(pagerank_df.withColumnRenamed(\"node\", \"target\").withColumnRenamed(\"pagerank\", \"dst_pr\"), on=\"target\", how=\"left\")\n",
    "data = data.join(degree_centrality_df.withColumnRenamed(\"node\", \"source\").withColumnRenamed(\"degree_centrality\", \"src_deg_centr\"), on=\"source\", how=\"left\")\n",
    "data = data.join(degree_centrality_df.withColumnRenamed(\"node\", \"target\").withColumnRenamed(\"degree_centrality\", \"dst_deg_centr\"), on=\"target\", how=\"left\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data.write.mode(\"overwrite\").parquet(WRITE_LOCATION)\n",
    "spark.stop()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
