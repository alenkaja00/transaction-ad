{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import os\n",
    "import shutil\n",
    "from datetime import datetime, timedelta\n",
    "import pandas as pd\n",
    "from pyspark.sql import functions as sf\n",
    "from pyspark import SparkConf\n",
    "from pyspark.sql import SparkSession"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "config = [\n",
    "    (\"spark.driver.memory\", \"10g\"), \n",
    "    (\"spark.jars.packages\", \"graphframes:graphframes:0.8.2-spark3.2-s_2.12\"),\n",
    "    (\"spark.executor.memory\", \"32g\"),\n",
    "    (\"spark.driver.memory\", \"64g\"),\n",
    "    (\"spark.driver.maxResultSize\", \"64g\"),\n",
    "    (\"spark.sql.session.timeZone\", \"UTC\")\n",
    "]\n",
    "spark = SparkSession.builder.appName(\"temporal_graph\").config(conf=SparkConf().setAll(config)).getOrCreate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"../config.json\") as f:\n",
    "    config = json.load(f)\n",
    "\n",
    "DATASET = config[\"dataset\"]\n",
    "WINDOW = config[\"temporal_graph\"]['window']\n",
    "\n",
    "transactions = pd.read_parquet(f\"../data/01-ibm-transactions-for-aml/preprocessed/{DATASET}-transactions\")\n",
    "transactions.rename(columns={\"transaction_id\": \"id\"}, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "transactions = transactions.sample(frac=0.1, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "DATA_FOLDER = \"../data/01-ibm-transactions-for-aml/temporal_graph\"\n",
    "location_staging = os.path.join(DATA_FOLDER, f\"{DATASET}_staging\")\n",
    "\n",
    "transactions[\"transaction_timestamp\"] = pd.to_datetime(transactions[\"timestamp\"])\n",
    "transactions[\"transaction_date\"] = transactions[\"transaction_timestamp\"].dt.date\n",
    "transactions[\"transaction_timestamp\"] = transactions[\"transaction_timestamp\"].astype(int) // 10**9\n",
    "del transactions[\"timestamp\"]\n",
    "\n",
    "transactions.to_parquet(location_staging)\n",
    "\n",
    "location_transactions = os.path.join(DATA_FOLDER, f\"{DATASET}_transactions\")\n",
    "staged = spark.read.parquet(location_staging)\n",
    "(\n",
    "    staged.repartition(\"transaction_date\")\n",
    "    .write.partitionBy(\"transaction_date\")\n",
    "    .mode(\"overwrite\")\n",
    "    .parquet(location_transactions)\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = spark.read.parquet(location_transactions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = data.withColumn(\"amount\", sf.ceil(\"amount\").cast(\"long\"))\n",
    "min_timestamp = data.select(sf.min(\"transaction_timestamp\")).collect()[0][0]\n",
    "data = data.withColumn(\"transaction_timestamp\", sf.col(\"transaction_timestamp\") - min_timestamp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = data.orderBy(\"transaction_timestamp\", \"transaction_date\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def rename_columns(dataframe, names):\n",
    "    for name, new_name in names.items():\n",
    "        dataframe = dataframe.withColumnRenamed(name, new_name)\n",
    "    return dataframe\n",
    "\n",
    "def max_timestamp(dt):\n",
    "    year, month, date = dt.split(\"-\")\n",
    "    return (datetime(int(year), int(month), int(date)) + timedelta(days=1)).timestamp()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "location_joins = os.path.join(DATA_FOLDER, f\"{DATASET}_joins\")\n",
    "shutil.rmtree(location_joins, ignore_errors=True)\n",
    "\n",
    "left_columns = {x: f\"{x}_left\" for x in data.columns}\n",
    "dates = data.select(\"transaction_date\").distinct().toPandas()\n",
    "dates = sorted([str(x) for x in dates[\"transaction_date\"].tolist()])\n",
    "for transaction_date in dates:\n",
    "    print(transaction_date)\n",
    "    start_index = dates.index(transaction_date)\n",
    "    end_index = start_index + WINDOW + 1\n",
    "    right_dates = dates[start_index:end_index]\n",
    "    end_date_max = str(pd.to_datetime(transaction_date).date() + timedelta(days=WINDOW))\n",
    "    right_dates = [x for x in right_dates if x <= end_date_max]\n",
    "    right = spark.read.option(\"basePath\", location_transactions).parquet(\n",
    "        *[f\"{location_transactions}{os.sep}transaction_date={x}\" for x in right_dates]\n",
    "    )\n",
    "    left = rename_columns(right.where(right.transaction_timestamp < max_timestamp(transaction_date)), left_columns)\n",
    "    join = left.join(right, left.target_left == right.source, \"inner\")\n",
    "    join = join.withColumn(\"delta\", join.transaction_timestamp - join.transaction_timestamp_left)\n",
    "    join = join.where(join.delta > 0)\n",
    "    join.write.parquet(f\"{location_joins}/staging_date={transaction_date}\", mode=\"overwrite\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "joins = spark.read.parquet(location_joins)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nodes_location = os.path.join(DATA_FOLDER, f\"{DATASET}_nodes\")\n",
    "edges_location = os.path.join(DATA_FOLDER, f\"{DATASET}_edges\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "node_columns = [\n",
    "    \"id\",\n",
    "    \"source\",\n",
    "    \"target\",\n",
    "    \"transaction_date\",\n",
    "    \"transaction_timestamp\",\n",
    "    \"amount\",\n",
    "]\n",
    "nodes = data.select(*node_columns).drop_duplicates(subset=[\"id\"])\n",
    "\n",
    "edges = joins.select(\n",
    "    sf.col(\"id_left\").alias(\"src\"),\n",
    "    sf.col(\"id\").alias(\"dst\"),\n",
    "    sf.col(\"transaction_date_left\").alias(\"src_date\"),\n",
    "    sf.col(\"transaction_date\").alias(\"dst_date\"),\n",
    "    (sf.when(\n",
    "        sf.col(\"amount_left\") > sf.col(\"amount\"), sf.col(\"amount\") / sf.col(\"amount_left\")\n",
    "    ).otherwise(sf.col(\"amount_left\") / sf.col(\"amount\"))).alias(\"weight\")\n",
    ")\n",
    "\n",
    "nodes = nodes.repartition(\"transaction_date\")\n",
    "nodes.write.partitionBy(\"transaction_date\").mode(\"overwrite\").parquet(nodes_location)\n",
    "\n",
    "partition_by = [\"src_date\", \"dst_date\"]\n",
    "edges.repartition(*partition_by).write.partitionBy(*partition_by).mode(\"overwrite\").parquet(edges_location)\n",
    "\n",
    "nodes = spark.read.parquet(nodes_location)\n",
    "edges = spark.read.parquet(edges_location)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nodes.show(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "edges.show(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"# of nodes\", nodes.count())\n",
    "print(\"# of edges\", edges.count())\n",
    "spark.stop()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
