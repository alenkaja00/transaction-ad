{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.preprocessing import MinMaxScaler, LabelEncoder\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch_geometric.data import Data\n",
    "import torch.nn.functional as F\n",
    "from torch_geometric.nn import SAGEConv, BatchNorm\n",
    "from tqdm import tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"../config.json\") as f:\n",
    "    config = json.load(f)\n",
    "\n",
    "DATASET = config[\"dataset\"]\n",
    "THRESHOLD = config[\"if_model\"][\"threshold\"]\n",
    "\n",
    "nodes_location = f\"../data/01-ibm-transactions-for-aml/temporal_graph/{DATASET}_nodes\"\n",
    "edges_location = f\"../data/01-ibm-transactions-for-aml/temporal_graph/{DATASET}_edges\"\n",
    "embeddings_location = f\"../data/01-ibm-transactions-for-aml/gnn_account/{DATASET}_account_embbedings.csv\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nodes_pd = pd.read_parquet(nodes_location)\n",
    "edges_pd = pd.read_parquet(edges_location)\n",
    "print(\"#nodes=\" + str(len(nodes_pd)) + \"\\n#edges=\" + str(len(edges_pd)))\n",
    "\n",
    "embeddings = pd.read_csv(embeddings_location, index_col=0)\n",
    "print(\"embeddings shape: \", embeddings.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "embeddings['emb_id'] = embeddings.index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nodes_pd = nodes_pd.merge(embeddings, left_on='source', right_on='emb_id', how='left').drop(columns='emb_id')\n",
    "nodes_pd = nodes_pd.merge(embeddings, left_on='target', right_on='emb_id', how='left').drop(columns='emb_id')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "edges_pd.drop(columns=[\"src_date\", \"dst_date\"], inplace=True)\n",
    "edges_pd.rename(columns={\"src\": \"source\", \"dst\": \"target\"}, inplace=True)\n",
    "nodes_pd.drop(columns=['source', 'target', 'transaction_date'], inplace=True)\n",
    "nodes_pd['transaction_timestamp'] = nodes_pd['transaction_timestamp'].astype(int)\n",
    "\n",
    "min_timestamp = nodes_pd['transaction_timestamp'].min()\n",
    "nodes_pd.loc[:, 'transaction_timestamp'] -= min_timestamp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "transactions = pd.read_parquet(f\"../data/01-ibm-transactions-for-aml/preprocessed/{DATASET}-transactions\")\n",
    "\n",
    "relevant_columns = ['transaction_id', 'source_currency', 'target_currency', 'format']\n",
    "transactions = transactions[relevant_columns]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "freq_enc = transactions['source_currency'].value_counts() / len(transactions)  \n",
    "transactions.loc[:, 'source_currency'] = transactions['source_currency'].map(freq_enc)\n",
    "\n",
    "freq_enc = transactions['target_currency'].value_counts() / len(transactions)  \n",
    "transactions.loc[:, 'target_currency'] = transactions['target_currency'].map(freq_enc)\n",
    "\n",
    "freq_enc = transactions['format'].value_counts() / len(transactions)  \n",
    "transactions.loc[:, 'format'] = transactions['format'].map(freq_enc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nodes_pd = nodes_pd.merge(transactions, left_on='id', right_on='transaction_id', how='left').drop(columns='transaction_id')\n",
    "\n",
    "nodes_pd['source_currency'] = nodes_pd['source_currency'].astype(np.float32)\n",
    "nodes_pd['target_currency'] = nodes_pd['target_currency'].astype(np.float32)\n",
    "nodes_pd['format'] = nodes_pd['format'].astype(np.float64)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "node_encoder = LabelEncoder()\n",
    "nodes_pd['node_id'] = node_encoder.fit_transform(nodes_pd['id'])\n",
    "edges_pd['source_id'] = node_encoder.transform(edges_pd['source'])\n",
    "edges_pd['target_id'] = node_encoder.transform(edges_pd['target'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "scaler_amount = MinMaxScaler()\n",
    "scaler_timestamp = MinMaxScaler()\n",
    "nodes_pd[\"amount\"] = scaler_amount.fit_transform(nodes_pd[[\"amount\"]])\n",
    "nodes_pd[\"transaction_timestamp\"] = scaler_timestamp.fit_transform(nodes_pd[[\"transaction_timestamp\"]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "normal = pd.read_csv(f\"../data/01-ibm-transactions-for-aml/filtered_output/normal_{DATASET}_{THRESHOLD}.csv\")\n",
    "anomalous = pd.read_csv(f\"../data/01-ibm-transactions-for-aml/filtered_output/non_normal_{DATASET}_{THRESHOLD}.csv\")\n",
    "normal_ids = list(set(normal[\"transaction_id\"].values))\n",
    "anomalous_ids = list(set(anomalous[\"transaction_id\"].values))\n",
    "\n",
    "normal_nodes = nodes_pd[nodes_pd[\"id\"].isin(normal_ids)]\n",
    "anomalous_nodes = nodes_pd[nodes_pd[\"id\"].isin(anomalous_ids)]\n",
    "\n",
    "normal_node_id_to_index = {node_id: idx for idx, node_id in enumerate(normal_nodes['id'].values)}\n",
    "anomalous_node_id_to_index = {node_id: idx for idx, node_id in enumerate(anomalous_nodes['id'].values)}\n",
    "\n",
    "normal_edges = edges_pd[edges_pd[\"source\"].isin(normal_nodes[\"id\"]) & edges_pd[\"target\"].isin(normal_nodes[\"id\"])]\n",
    "anomalous_edges = edges_pd[edges_pd[\"source\"].isin(anomalous_nodes[\"id\"]) & edges_pd[\"target\"].isin(anomalous_nodes[\"id\"])]\n",
    "\n",
    "normal_edge_index_np = np.array([\n",
    "    [normal_node_id_to_index[src], normal_node_id_to_index[dst]] \n",
    "    for src, dst in zip(normal_edges['source'].values, normal_edges['target'].values)\n",
    "], dtype=np.int32).T\n",
    "\n",
    "anomalous_edge_index_np = np.array([\n",
    "    [anomalous_node_id_to_index[src], anomalous_node_id_to_index[dst]] \n",
    "    for src, dst in zip(anomalous_edges['source'].values, anomalous_edges['target'].values)\n",
    "], dtype=np.int32).T\n",
    "\n",
    "normal_edge_index = torch.tensor(normal_edge_index_np, dtype=torch.long)\n",
    "anomalous_edge_index = torch.tensor(anomalous_edge_index_np, dtype=torch.long)\n",
    "\n",
    "normal_edge_attr = torch.tensor(normal_edges['weight'].values, dtype=torch.float).unsqueeze(1)\n",
    "anomalous_edge_attr = torch.tensor(anomalous_edges['weight'].values, dtype=torch.float).unsqueeze(1)\n",
    "\n",
    "normal_node_features = normal_nodes.drop(columns=['id', 'node_id']).values\n",
    "anomalous_node_features = anomalous_nodes.drop(columns=['id', 'node_id']).values\n",
    "\n",
    "scaler = MinMaxScaler()\n",
    "normal_node_features = scaler.fit_transform(normal_node_features)\n",
    "anomalous_node_features = scaler.transform(anomalous_node_features)\n",
    "\n",
    "normal_node_features = torch.tensor(normal_node_features, dtype=torch.float)\n",
    "anomalous_node_features = torch.tensor(anomalous_node_features, dtype=torch.float)\n",
    "\n",
    "train_data = Data(x=normal_node_features, edge_index=normal_edge_index, edge_attr=normal_edge_attr)\n",
    "test_data = Data(x=anomalous_node_features, edge_index=anomalous_edge_index, edge_attr=anomalous_edge_attr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class GraphSAGEAutoencoder(torch.nn.Module):\n",
    "    def __init__(self, in_channels, hidden_channels, out_channels, num_layers=2, dropout=0.5):\n",
    "        super(GraphSAGEAutoencoder, self).__init__()\n",
    "        self.encoder = torch.nn.ModuleList()\n",
    "        self.bns = torch.nn.ModuleList()\n",
    "        \n",
    "        self.encoder.append(SAGEConv(in_channels, hidden_channels))\n",
    "        self.bns.append(torch.nn.BatchNorm1d(hidden_channels))\n",
    "        \n",
    "        for _ in range(num_layers - 2):\n",
    "            self.encoder.append(SAGEConv(hidden_channels, hidden_channels))\n",
    "            self.bns.append(torch.nn.BatchNorm1d(hidden_channels))\n",
    "        \n",
    "        self.encoder.append(SAGEConv(hidden_channels, out_channels))\n",
    "        self.dropout = dropout\n",
    "        \n",
    "        self.decoder = torch.nn.Linear(out_channels, in_channels)\n",
    "\n",
    "    def forward(self, data):\n",
    "        x, edge_index = data.x, data.edge_index\n",
    "        for conv, bn in zip(self.encoder[:-1], self.bns):\n",
    "            x = conv(x, edge_index)\n",
    "            x = bn(x)\n",
    "            x = F.relu(x)\n",
    "            x = F.dropout(x, p=self.dropout, training=self.training)\n",
    "        embeddings = self.encoder[-1](x, edge_index)\n",
    "        reconstructed = self.decoder(embeddings)\n",
    "        return reconstructed, embeddings\n",
    "\n",
    "\n",
    "model = GraphSAGEAutoencoder(\n",
    "    in_channels=train_data.num_node_features, \n",
    "    hidden_channels=32, \n",
    "    out_channels=16, \n",
    "    num_layers=2, \n",
    "    dropout=0.2\n",
    ")\n",
    "optimizer = torch.optim.AdamW(model.parameters(), lr=0.01, weight_decay=1e-4)\n",
    "\n",
    "def reconstruction_loss(reconstructed, original):\n",
    "    return F.mse_loss(reconstructed, original)\n",
    "\n",
    "def train_autoencoder(data, model, num_epochs=200):\n",
    "    model.train()\n",
    "    scheduler = torch.optim.lr_scheduler.StepLR(optimizer, step_size=50, gamma=0.5)\n",
    "    \n",
    "    for epoch in tqdm(range(num_epochs), desc=\"Training Epochs\"):\n",
    "        optimizer.zero_grad()\n",
    "        \n",
    "        reconstructed, embeddings = model(data)\n",
    "        loss = reconstruction_loss(reconstructed, data.x)\n",
    "        \n",
    "        loss.backward()\n",
    "        torch.nn.utils.clip_grad_norm_(model.parameters(), max_norm=1.0)\n",
    "        optimizer.step()\n",
    "        \n",
    "        if epoch % 10 == 0:\n",
    "            print(f'Epoch {epoch} -> Loss: {loss.item()}')\n",
    "        scheduler.step()\n",
    "    \n",
    "    return model, embeddings\n",
    "\n",
    "\n",
    "model, embeddings = train_autoencoder(train_data, model, num_epochs=40)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.eval()\n",
    "with torch.no_grad():\n",
    "    reconstructed, test_embeddings = model(test_data)\n",
    "\n",
    "reconstruction_errors = torch.mean((reconstructed - test_data.x) ** 2, dim=1).cpu().numpy()\n",
    "\n",
    "ranked_indices = np.argsort(reconstruction_errors)[::-1]\n",
    "\n",
    "top_k = 5000\n",
    "top_k_anomalous_nodes = ranked_indices[:top_k]\n",
    "top_k_anomalous_node_ids = anomalous_nodes['id'].iloc[top_k_anomalous_nodes]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "transactions = pd.read_parquet(f\"../data/01-ibm-transactions-for-aml/preprocessed/{DATASET}-transactions\")\n",
    "real_laundering_ids = transactions[transactions['is_laundering'] == 1]['transaction_id'].values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "true_positives = len(set(real_laundering_ids) & set(top_k_anomalous_node_ids))\n",
    "false_positives = top_k - true_positives\n",
    "\n",
    "print(f\"True Positives: {true_positives}\")\n",
    "print(f\"False Positives: {false_positives}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "threshold = np.percentile(reconstruction_errors, 90)\n",
    "anomalous_transactions = anomalous_nodes[reconstruction_errors > threshold]\n",
    "anomalous_transactions_ids = anomalous_transactions['id'].values\n",
    "\n",
    "print(f\"Threshold: {threshold}\")\n",
    "print(f\"Anomalous Transactions: {len(anomalous_transactions_ids)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"TP:\", len(set(anomalous_transactions_ids) & set(real_laundering_ids)))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
