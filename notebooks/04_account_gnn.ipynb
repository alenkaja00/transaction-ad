{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import networkx as nx\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.preprocessing import MinMaxScaler, LabelEncoder, MinMaxScaler\n",
    "import torch\n",
    "import torch.nn.functional as F\n",
    "from torch_geometric.nn import SAGEConv\n",
    "from torch_geometric.data import Data\n",
    "from torch_geometric.loader import NeighborLoader\n",
    "from torch_geometric.utils import negative_sampling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"../config.json\") as f:\n",
    "    config = json.load(f)  \n",
    "\n",
    "DATASET = config[\"dataset\"]\n",
    "THRESHOLD = config[\"if_model\"][\"threshold\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "transactions = pd.read_parquet(f\"../data/01-ibm-transactions-for-aml/preprocessed/{DATASET}-transactions\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_aggregated_all = transactions.groupby([\"source\", \"target\"]).agg(\n",
    "        sent_amount=pd.NamedAgg(column=\"amount\", aggfunc=\"sum\"),\n",
    "        received_amount=pd.NamedAgg(column=\"amount\", aggfunc=\"sum\")\n",
    ").reset_index()\n",
    "\n",
    "mapping_source = data_aggregated_all.groupby(\"source\").agg({\"sent_amount\": \"sum\"})[\"sent_amount\"].to_dict()\n",
    "mapping_target = data_aggregated_all.groupby(\"target\").agg({\"received_amount\": \"sum\"})[\"received_amount\"].to_dict()\n",
    "\n",
    "data_aggregated_all[\"total_sent_by_source\"] = data_aggregated_all[\"source\"].map(mapping_source)\n",
    "data_aggregated_all[\"total_received_by_target\"] = data_aggregated_all[\"target\"].map(mapping_target)\n",
    "\n",
    "data_aggregated_all[\"weight\"] = data_aggregated_all.apply(\n",
    "    lambda x: (\n",
    "        x[\"sent_amount\"] / x[\"total_sent_by_source\"] + \n",
    "        x[\"received_amount\"] / x[\"total_received_by_target\"]\n",
    "    ), axis=1\n",
    ")\n",
    "\n",
    "edges = data_aggregated_all.loc[:, [\"source\", \"target\", \"weight\", \"sent_amount\"]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "normal = pd.read_csv(f\"../data/01-ibm-transactions-for-aml/filtered_output/normal_{DATASET}_{THRESHOLD}.csv\")\n",
    "anomalous = pd.read_csv(f\"../data/01-ibm-transactions-for-aml/filtered_output/non_normal_{DATASET}_{THRESHOLD}.csv\")\n",
    "\n",
    "normal_ids = set(normal[\"transaction_id\"].values)\n",
    "anomalous_ids = set(anomalous[\"transaction_id\"].values)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nodes = transactions[\"source\"].unique()\n",
    "nodes = np.append(nodes, transactions[\"target\"].unique())\n",
    "nodes = np.unique(nodes)\n",
    "nodes = pd.DataFrame(nodes, columns=[\"node\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "transactions[\"if_anomaly\"] = np.where(transactions[\"transaction_id\"].isin(anomalous_ids), 1, 0)\n",
    "\n",
    "suspicious_by_source = transactions.groupby(\"source\").agg({\"if_anomaly\": \"sum\"}).reset_index()\n",
    "suspicious_by_target = transactions.groupby(\"target\").agg({\"if_anomaly\": \"sum\"}).reset_index()\n",
    "\n",
    "total_by_source = transactions.groupby(\"source\").agg({\"transaction_id\": \"count\"}).reset_index()\n",
    "total_by_target = transactions.groupby(\"target\").agg({\"transaction_id\": \"count\"}).reset_index()\n",
    "\n",
    "suspicious_by_source[\"suspicious_by_source\"] = suspicious_by_source[\"if_anomaly\"] / total_by_source[\"transaction_id\"]\n",
    "suspicious_by_target[\"suspicious_by_target\"] = suspicious_by_target[\"if_anomaly\"] / total_by_target[\"transaction_id\"]\n",
    "\n",
    "suspicious_by_source = suspicious_by_source.loc[:, [\"source\", \"suspicious_by_source\"]]\n",
    "suspicious_by_target = suspicious_by_target.loc[:, [\"target\", \"suspicious_by_target\"]]\n",
    "suspicious_by_source.columns = [\"node\", \"suspicious_by_source\"]\n",
    "suspicious_by_target.columns = [\"node\", \"suspicious_by_target\"]\n",
    "\n",
    "nodes = pd.merge(nodes, suspicious_by_source, on=\"node\", how=\"left\").fillna(0)\n",
    "nodes = pd.merge(nodes, suspicious_by_target, on=\"node\", how=\"left\").fillna(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nodes[\"total_sent\"] = nodes[\"node\"].map(mapping_source).fillna(0)\n",
    "nodes[\"total_received\"] = nodes[\"node\"].map(mapping_target).fillna(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "G = nx.from_pandas_edgelist(edges, source='source', target='target', edge_attr=True, create_using=nx.DiGraph())\n",
    "in_degree = dict(G.in_degree())\n",
    "out_degree = dict(G.out_degree())\n",
    "pagerank = nx.pagerank(G, weight='weight')\n",
    "clustering_coefficient = nx.clustering(G.to_undirected(), weight='weight')\n",
    "hits_hubs, hits_authorities = nx.hits(G, max_iter=100, tol=1.0e-8, nstart=None, normalized=True)\n",
    "node_metrics = pd.DataFrame({\n",
    "    'node': list(G.nodes()),\n",
    "    'in_degree': pd.Series(in_degree),\n",
    "    'out_degree': pd.Series(out_degree),\n",
    "    'pagerank': pd.Series(pagerank),\n",
    "    'clust_coefficient': pd.Series(clustering_coefficient),\n",
    "    'hits_hubs': pd.Series(hits_hubs),\n",
    "    'hits_auth': pd.Series(hits_authorities)\n",
    "})\n",
    "\n",
    "nodes = pd.merge(nodes, node_metrics, on='node', how='left')\n",
    "\n",
    "nodes['inflow_ratio'] = nodes['total_received'] / (nodes['total_sent'] + nodes['total_received'])\n",
    "nodes['outflow_ratio'] = nodes['total_sent'] / (nodes['total_sent'] + nodes['total_received'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "avg_sent_by_node = transactions.groupby('source').agg({'amount': 'mean'}).reset_index()\n",
    "avg_sent_by_node.columns = ['node', 'avg_sent']\n",
    "nodes = pd.merge(nodes, avg_sent_by_node, on='node', how='left')\n",
    "\n",
    "avg_received_by_node = transactions.groupby('target').agg({'amount': 'mean'}).reset_index()\n",
    "avg_received_by_node.columns = ['node', 'avg_received']\n",
    "nodes = pd.merge(nodes, avg_received_by_node, on='node', how='left')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "count = transactions.groupby(['source', 'target']).size().reset_index(name='count')\n",
    "edges = pd.merge(edges, count, on=['source', 'target'], how='left')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nodes = nodes.fillna(0)\n",
    "edges = edges.fillna(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "node_columns_log_transform = ['total_sent', 'total_received', 'avg_sent', 'avg_received', 'in_degree', 'out_degree']\n",
    "\n",
    "node_columns_min_max = ['pagerank', 'clust_coefficient', 'hits_hubs', 'hits_auth']\n",
    "\n",
    "edge_columns_log_transform = ['sent_amount']\n",
    "edge_columns_min_max = ['weight', 'count']\n",
    "\n",
    "scaler = MinMaxScaler()\n",
    "\n",
    "for col in node_columns_log_transform:\n",
    "    nodes[col] = np.log1p(nodes[col])\n",
    "\n",
    "nodes[node_columns_log_transform + node_columns_min_max] = scaler.fit_transform(\n",
    "    nodes[node_columns_log_transform + node_columns_min_max])\n",
    "\n",
    "for col in edge_columns_log_transform:\n",
    "    edges[col] = np.log1p(edges[col])\n",
    "\n",
    "edges[edge_columns_log_transform + edge_columns_min_max] = scaler.fit_transform(\n",
    "    edges[edge_columns_log_transform + edge_columns_min_max])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nodes_location = f\"../data/01-ibm-transactions-for-aml/gnn_account/{DATASET}_nodes.csv\"\n",
    "edges_location = f\"../data/01-ibm-transactions-for-aml/gnn_account/{DATASET}_edges.csv\"\n",
    "\n",
    "nodes.to_csv(nodes_location, index=False)\n",
    "edges.to_csv(edges_location, index=False)\n",
    "\n",
    "nodes = pd.read_csv(nodes_location)\n",
    "edges = pd.read_csv(edges_location)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "node_encoder = LabelEncoder()\n",
    "nodes['node_id'] = node_encoder.fit_transform(nodes['node'])\n",
    "edges['source_id'] = node_encoder.transform(edges['source'])\n",
    "edges['target_id'] = node_encoder.transform(edges['target'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "node_features = nodes.drop(columns=['node', 'node_id']).values\n",
    "node_features = torch.tensor(node_features, dtype=torch.float)\n",
    "\n",
    "edge_index_np = np.array([edges['source_id'].values, edges['target_id'].values], dtype=np.int64)\n",
    "edge_index = torch.tensor(edge_index_np, dtype=torch.long)\n",
    "edge_attr = torch.tensor(edges['weight'].values, dtype=torch.float).unsqueeze(1)\n",
    "\n",
    "\n",
    "data = Data(x=node_features, edge_index=edge_index, edge_attr=edge_attr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class GraphSAGE(torch.nn.Module):\n",
    "    def __init__(self, in_channels, hidden_channels, out_channels):\n",
    "        super(GraphSAGE, self).__init__()\n",
    "        self.conv1 = SAGEConv(in_channels, hidden_channels)\n",
    "        self.conv2 = SAGEConv(hidden_channels, out_channels)\n",
    "        self.reset_parameters()\n",
    "\n",
    "    def reset_parameters(self):\n",
    "        self.conv1.reset_parameters()\n",
    "        self.conv2.reset_parameters()\n",
    "\n",
    "    def forward(self, x, edge_index):\n",
    "        x = self.conv1(x, edge_index)\n",
    "        x = F.relu(x)\n",
    "        x = F.normalize(x, p=2, dim=-1)\n",
    "        x = self.conv2(x, edge_index)\n",
    "        x = F.normalize(x, p=2, dim=-1)\n",
    "        return x\n",
    "\n",
    "\n",
    "model = GraphSAGE(in_channels=data.num_features, hidden_channels=64, out_channels=16)\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=0.01)\n",
    "\n",
    "loader = NeighborLoader(data, num_neighbors=[25, 10], batch_size=256)\n",
    "\n",
    "epsilon = 1e-15\n",
    "\n",
    "def unsupervised_loss(z, pos_edge_index, neg_edge_index):\n",
    "    pos_loss = -torch.log(torch.sigmoid((z[pos_edge_index[0]] * z[pos_edge_index[1]]).sum(dim=-1)) + epsilon).mean()\n",
    "    neg_loss = -torch.log(1 - torch.sigmoid((z[neg_edge_index[0]] * z[neg_edge_index[1]]).sum(dim=-1)) + epsilon).mean()\n",
    "    return pos_loss + neg_loss\n",
    "\n",
    "\n",
    "model.train()\n",
    "for epoch in range(50):\n",
    "    total_loss = 0\n",
    "    for batch in loader:\n",
    "        optimizer.zero_grad()\n",
    "        z = model(batch.x, batch.edge_index)\n",
    "        \n",
    "        pos_edge_index = batch.edge_index\n",
    "        neg_edge_index = negative_sampling(edge_index=batch.edge_index, num_nodes=batch.num_nodes, num_neg_samples=pos_edge_index.size(1))\n",
    "        \n",
    "        loss = unsupervised_loss(z, pos_edge_index, neg_edge_index)\n",
    "        loss.backward()\n",
    "        torch.nn.utils.clip_grad_norm_(model.parameters(), max_norm=1.0)\n",
    "        optimizer.step()\n",
    "        total_loss += loss.item()\n",
    "    print(f'Epoch {epoch+1}, Loss: {total_loss/len(loader)}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.eval()\n",
    "with torch.no_grad():\n",
    "    embeddings = model(data.x, data.edge_index)\n",
    "\n",
    "embeddings = embeddings.detach().cpu().numpy()\n",
    "\n",
    "original_node_ids = node_encoder.inverse_transform(range(embeddings.shape[0]))\n",
    "embeddings_df = pd.DataFrame(embeddings, index=original_node_ids, columns=[f'emb_{i}' for i in range(embeddings.shape[1])])\n",
    "\n",
    "embeddings_df.to_csv(f\"../data/01-ibm-transactions-for-aml/gnn_account/{DATASET}_account_embbedings.csv\", index=True)\n",
    "print(\"embeddings saved\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
